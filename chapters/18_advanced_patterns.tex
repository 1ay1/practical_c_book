\chapter{Advanced Patterns: The Deep Magic}

\section{The Power of X-Macros Revisited}

X-Macros are one of C's most powerful meta-programming tools. Let's explore advanced uses:

\begin{lstlisting}
// Define a complete subsystem with one list
#define COMMANDS \
    X(quit,   "q",  "Exit program",        cmd_quit) \
    X(help,   "h",  "Show help",           cmd_help) \
    X(save,   "s",  "Save current state",  cmd_save) \
    X(load,   "l",  "Load saved state",    cmd_load) \
    X(list,   "ls", "List items",          cmd_list) \
    X(add,    "a",  "Add new item",        cmd_add)

// Generate enum
#define X(name, short_cmd, desc, func) CMD_##name,
typedef enum {
    COMMANDS
    CMD_COUNT
} Command;
#undef X

// Generate function prototypes
#define X(name, short_cmd, desc, func) \
    void func(const char* args);
COMMANDS
#undef X

// Generate dispatch table
#define X(name, short_cmd, desc, func) \
    {#name, short_cmd, desc, func},
typedef struct {
    const char* name;
    const char* short_name;
    const char* description;
    void (*handler)(const char*);
} CommandEntry;

CommandEntry command_table[] = {
    COMMANDS
};
#undef X

// Generate help text
void print_help(void) {
    printf("Available commands:\n");
#define X(name, short_cmd, desc, func) \
    printf("  %-10s (%-3s) - %s\n", #name, short_cmd, desc);
    COMMANDS
#undef X
}

// Generate command name lookup
const char* command_name(Command cmd) {
#define X(name, short_cmd, desc, func) #name,
    static const char* names[] = { COMMANDS };
#undef X
    return names[cmd];
}
\end{lstlisting}

\section{Coroutines in C}

\noindent\rule{\textwidth}{0.4pt}

Coroutines provide cooperative multitasking without the overhead of threads or the complexity of callbacks. They allow functions to suspend execution and resume later, maintaining their local state between invocations. While C lacks native coroutine support, several clever techniques simulate this behavior.

\begin{notebox}
\textbf{What You'll Learn:} This section explores stackless coroutine implementations in C, from Simon Tatham's elegant macro-based approach to explicit state machines. You'll see practical applications in protocol parsing, generators, and async I/O.
\end{notebox}

\subsection{Understanding Coroutines}

Before diving into implementation, we must understand what makes coroutines fundamentally different from ordinary functions. A normal function has a simple lifecycle: it begins execution, runs to completion, and returns. All local variables are destroyed when the function exits. Each call starts fresh with no memory of previous invocations.

Coroutines break this model entirely. They introduce the concept of \textit{suspendable execution}---a function that can pause in the middle, return control to its caller, and later resume exactly where it left off. This seemingly simple change has profound implications for how we structure code.

Coroutines differ from regular functions in key ways:

\begin{description}[style=nextline,leftmargin=0pt]
    \item[\textbf{Suspendable}] Can pause execution and return control to caller. Unlike a normal \texttt{return}, which destroys the function's context, a coroutine yield preserves everything. The instruction pointer, local variables, and execution state remain alive but dormant.

    \item[\textbf{Resumable}] Can continue from where they left off. When called again, the coroutine doesn't start from the beginning. Instead, it resumes immediately after the last yield point, as if it never stopped.

    \item[\textbf{State Preservation}] Maintain local variables across invocations. This is the key challenge in C. Languages with native coroutine support handle this automatically, but in C, we must explicitly preserve state between calls using static variables or context structures.

    \item[\textbf{Cooperative}] Explicitly yield control (unlike preemptive threads). The coroutine decides when to suspend. This eliminates race conditions and the need for locks, but requires careful design to avoid one coroutine monopolizing CPU time.
\end{description}

The power of coroutines becomes apparent when dealing with complex state machines, parsers, or any algorithm that naturally involves multiple stages. Instead of writing explicit state tracking code with switch statements and state variables, the coroutine's execution flow itself represents the state. This makes code more readable and maintainable.

\subsection{Simon Tatham's Coroutine Macros}

\begin{tipbox}
\textbf{The Elegant Solution:} Simon Tatham's coroutine macros represent one of the most clever uses of C preprocessor magic. By combining Duff's Device with \texttt{\_\_LINE\_\_}, they create automatic state machines with minimal boilerplate.
\end{tipbox}

\vspace{0.3cm}

The most elegant stackless coroutine implementation uses Duff's Device and the \texttt{\_\_LINE\_\_} macro. This technique, devised by Simon Tatham, is a masterpiece of macro engineering. It exploits an obscure interaction between C's switch statement and the preprocessor to create automatic state machines.

\textbf{The Fundamental Insight:} C allows case labels anywhere within a switch statement, even nested inside other constructs like loops. Combined with the \texttt{\_\_LINE\_\_} macro (which expands to the current source line number), we can create unique state identifiers automatically. Each yield point gets a different line number, providing a natural way to track where execution should resume.

\begin{lstlisting}
// Coroutine macros using line numbers for state
#define crBegin static int state=0; switch(state) { case 0:
#define crReturn(x) do { state=__LINE__; return x; \
                         case __LINE__:; } while(0)
#define crFinish }

// Simple example: Generate Fibonacci numbers
int fibonacci(void) {
    static int a = 0, b = 1;

    crBegin;

    while(1) {
        crReturn(a);
        int temp = a;
        a = b;
        b = temp + b;
    }

    crFinish;
    return 0;
}

// Usage
for(int i = 0; i < 10; i++) {
    printf("%d ", fibonacci());  // 0 1 1 2 3 5 8 13 21 34
}
\end{lstlisting}

\vspace{0.3cm}
\noindent\textbf{Understanding the Parser:}

This parser demonstrates several important coroutine patterns. First, notice how the control flow reads naturally from top to bottom, just like you'd describe the protocol in English: ``read the header until you find a blank line, extract the content length, allocate a buffer, read the body, then process the request.''

The \texttt{crReturn} calls represent points where we need more data. In a traditional blocking implementation, these would be blocking reads. In a callback-based implementation, each would be a separate function. Here, they're simple yield points---the function pauses, returns control to the caller (who presumably will provide more data), and resumes when called again.

The static variables preserve all state: where we are in the header, how much body we've read, what the content length is. This is essential because each call to the parser might provide only one byte of data. The coroutine accumulates this data incrementally, maintaining perfect knowledge of its progress through the protocol.

Error handling becomes more natural too. Instead of propagating error codes through multiple callback functions, we can simply return an ERROR state and reset. The sequential flow makes it easy to see the happy path and the error conditions.

\begin{warningbox}
\textbf{Memory Management Caveat:} Notice that we allocate \texttt{body} with malloc and must remember to free it. In a more robust implementation, you'd want cleanup logic that runs even if the parser is abandoned mid-stream. This is one area where stackless coroutines show their limitations---you can't rely on automatic cleanup like you would with scope-based resource management.
\end{warningbox}

\vspace{0.3cm}
\noindent\textbf{\large How It Works: A Deep Dive}

\vspace{0.2cm}
\noindent Let's dissect this mechanism in detail, because understanding it requires thinking about C preprocessing and control flow simultaneously.

\begin{enumerate}[leftmargin=*]
    \item \textbf{First call:} When the function first executes, the static variable \texttt{state} is initialized to 0. The \texttt{crBegin} macro expands to declare this variable and open a switch statement with \texttt{case 0:}. Since \texttt{state} is 0, execution begins at this case label and proceeds normally.

    \item \textbf{Yielding:} When \texttt{crReturn} executes, the preprocessor replaces \texttt{\_\_LINE\_\_} with the current source line number. This number is stored in \texttt{state}. The macro then returns from the function with the specified value. Crucially, because \texttt{state} is static, it persists after the function returns.

    \item \textbf{Next call:} On the subsequent call, \texttt{state} still holds the line number from the previous yield. The switch statement now jumps directly to the case label with that line number. Because \texttt{crReturn} places a case label immediately after the return statement, execution resumes right where it left off.

    \item \textbf{Static variables:} All state (like \texttt{a} and \texttt{b} in the Fibonacci example) must be static. This is the price of stackless coroutines---we cannot rely on the normal function call stack. Instead, we explicitly persist everything we need between invocations.
\end{enumerate}

\begin{warningbox}
\textbf{Stackless Trade-off:} This technique is called ``stackless'' because it doesn't manipulate the actual call stack. You gain simplicity and portability, but lose automatic variable preservation. Every piece of state must be explicitly declared as static.
\end{warningbox}

\vspace{0.2cm}
\noindent\textit{The genius of this approach is that the state machine is implicit.} You write code that looks like normal sequential logic, and the macros transform it into a state machine at compile time. The alternative---hand-coding the state machine---is error-prone and obscures the algorithm's logic.

\vspace{0.5cm}
\subsection{Protocol State Machine Example}

\noindent\rule{\textwidth}{0.4pt}
\vspace{0.2cm}

Coroutines excel at implementing complex protocols without callback hell. Traditional callback-based approaches force you to split your logic across multiple functions, each handling one stage of the protocol. State must be passed around in context structures, and the overall flow becomes hard to follow.

\textbf{The Advantage:} With coroutines, the entire protocol implementation lives in one function, written as straightforward sequential code. This is a game-changer for protocol implementations, parsers, and state machines. Let's examine a realistic protocol parser that demonstrates these advantages:

\begin{lstlisting}
typedef enum { WAITING, READING_HEADER, READING_BODY,
               PROCESSING, COMPLETE, ERROR } State;

// HTTP-like protocol parser as coroutine
State http_parser(char* input, int len) {
    static int state = 0;
    static char header[256];
    static int header_pos = 0;
    static int content_length = 0;
    static int body_pos = 0;
    static char* body = NULL;

    crBegin;

    // Read header until blank line
    header_pos = 0;
    while(1) {
        crReturn(READING_HEADER);

        if(input[0] == '\n' && header_pos > 0 &&
           header[header_pos-1] == '\n') {
            header[header_pos] = '\0';
            break;
        }

        if(header_pos < sizeof(header)-1) {
            header[header_pos++] = input[0];
        }
    }

    // Extract content length
    content_length = parse_content_length(header);
    if(content_length <= 0) {
        crReturn(ERROR);
    }

    // Allocate and read body
    body = malloc(content_length);
    body_pos = 0;

    while(body_pos < content_length) {
        crReturn(READING_BODY);
        body[body_pos++] = input[0];
    }

    // Process complete request
    process_request(header, body, content_length);
    free(body);

    crReturn(COMPLETE);

    // Reset for next request
    state = 0;

    crFinish;
    return ERROR;
}
\end{lstlisting}

\vspace{0.3cm}
\noindent\textbf{Analyzing the Implementation:}

This example illustrates the explicit approach's flexibility. The \texttt{CoroContext} structure contains all state: the current position in the state machine (\texttt{state}), loop counters (\texttt{i}, \texttt{j}), accumulated results (\texttt{total}), and buffers.

The state machine has clear stages: initialization (state 0), input collection (state 1), processing (state 2), and output (state 3). Each state does a small amount of work and returns, allowing the caller to interleave multiple coroutines or respond to other events.

Notice the fall-through behavior between some states (using comments to indicate this). State 0 initializes and immediately falls into state 1. This is deliberate---initialization completes instantly, so we don't need to yield. State 3, after outputting results, resets to state 0 for the next cycle.

The processing stage (state 2) demonstrates ``yielding in a loop.'' It processes one character per call, yielding between each. This allows the coroutine to make incremental progress without blocking. In a real application, this might represent a computationally expensive operation that we want to spread over multiple frames or time slices.

The return values (\texttt{CORO\_YIELDED} vs \texttt{CORO\_DONE}) inform the caller about the coroutine's status. This is more explicit than Simon Tatham's approach, where the return value typically carries application data. Here, we separate status from data, making the protocol cleaner.

Multiple instances work naturally: just allocate multiple \texttt{CoroContext} structures. Each maintains independent state. This is perfect for scenarios like handling multiple network connections, where each connection needs its own parser coroutine.

\vspace{0.5cm}
\subsection{Explicit State Structure Approach}

\noindent\rule{\textwidth}{0.4pt}
\vspace{0.2cm}

For more complex scenarios, explicit state management provides better control. While Simon Tatham's macros are elegant for simple cases, they have limitations: all state must be static (preventing multiple coroutine instances), and the macro magic can be hard to debug.

\begin{tipbox}
\textbf{When to Go Explicit:} Use explicit state structures when you need multiple coroutine instances, better debuggability, or fine-grained control over memory management. The trade-off is more boilerplate for transparency and flexibility.
\end{tipbox}

\vspace{0.2cm}
\noindent An alternative approach uses explicit state structures. This is more verbose but offers significant advantages: you can have multiple coroutine instances, the state is visible and debuggable, and you have complete control over memory management and initialization.

This approach effectively hand-codes what the macros generate automatically. You explicitly number your states and write the switch statement yourself.

\begin{lstlisting}
typedef struct {
    int state;
    // Coroutine-specific state
    int i, j;
    int total;
    char buffer[256];
    size_t buffer_pos;
} CoroContext;

typedef enum { CORO_RUNNING, CORO_YIELDED, CORO_DONE } CoroStatus;

// Initialize coroutine
void coro_init(CoroContext* ctx) {
    memset(ctx, 0, sizeof(*ctx));
}

// Multi-stage data processor
CoroStatus data_processor(CoroContext* ctx, char input) {
    switch(ctx->state) {
        case 0:  // Initialization
            ctx->total = 0;
            ctx->buffer_pos = 0;
            ctx->state = 1;
            // Fall through

        case 1:  // Collect input until newline
            if(input == '\n') {
                ctx->buffer[ctx->buffer_pos] = '\0';
                ctx->state = 2;
                ctx->i = 0;
                return CORO_YIELDED;
            }

            if(ctx->buffer_pos < sizeof(ctx->buffer) - 1) {
                ctx->buffer[ctx->buffer_pos++] = input;
            }
            return CORO_YIELDED;

        case 2:  // Process buffer (simulate slow operation)
            // Process one character at a time, yielding between
            while(ctx->i < ctx->buffer_pos) {
                ctx->total += ctx->buffer[ctx->i];
                ctx->i++;
                return CORO_YIELDED;  // Yield after each char
            }
            ctx->state = 3;
            // Fall through

        case 3:  // Output result
            printf("Processed: %s (sum=%d)\n",
                   ctx->buffer, ctx->total);
            ctx->state = 0;  // Reset
            return CORO_DONE;
    }

    return CORO_DONE;
}

// Usage: Process input incrementally
CoroContext ctx;
coro_init(&ctx);

const char* inputs = "Hello\nWorld\n";
for(size_t i = 0; i < strlen(inputs); i++) {
    CoroStatus status = data_processor(&ctx, inputs[i]);
    if(status == CORO_DONE) {
        coro_init(&ctx);  // Start new processing cycle
    }
}
\end{lstlisting}

The prime generator showcases a more sophisticated example. It maintains a growing list of discovered primes, using them to test future candidates. This is a form of the Sieve of Eratosthenes, but implemented as a generator rather than a batch algorithm.

Each call to \texttt{prime\_next} does just enough work to find one prime. The state persists between calls: the current candidate number, all previously discovered primes, and where we are in the testing process. This allows the caller to request primes one at a time, stopping whenever they have enough.

The optimization inside the divisibility check is worth noting. We only test divisors up to the square root of the candidate (checked by \texttt{primes[i] * primes[i] > candidate}). This dramatically reduces the number of divisions needed, especially for large primes.

Memory management is explicit here. The generator allocates and reallocates its internal prime list as needed, using a doubling strategy for amortized O(1) insertions. The caller must call \texttt{prime\_free} when done. This is manual but gives complete control over allocations.

The key advantage over generating all primes upfront is flexibility. If you need the first million primes, a generator produces them incrementally, allowing processing to overlap with generation. If you only need primes until you find one meeting some condition, you can stop early without wasting computation. The generator's state is suspended, ready to continue if needed.

This pattern extends to many scenarios: walking tree structures, generating permutations, producing infinite sequences, or reading large files line-by-line. The coroutine maintains complex traversal state while presenting a simple ``give me the next item'' interface.

\vspace{0.5cm}
\subsection{Generator Pattern}

\noindent\rule{\textwidth}{0.4pt}
\vspace{0.2cm}

Coroutines naturally implement generators---functions that produce a sequence of values over time rather than all at once. Languages like Python and JavaScript have native generator syntax, but C requires manual implementation. Coroutines provide an elegant way to achieve similar behavior.

\begin{notebox}
\textbf{Key Insight:} A generator is just a coroutine that yields values. Instead of yielding to wait for input (like parsers), a generator yields to provide output. Each call produces the next value in the sequence.
\end{notebox}

\vspace{0.2cm}
\noindent This pattern is incredibly useful for iteration, lazy evaluation, and working with sequences too large to fit in memory. Instead of generating an entire array upfront (which might be millions of elements), a generator produces values on demand.

\begin{lstlisting}
// Range generator with step
typedef struct {
    int current;
    int end;
    int step;
    int state;
} RangeGenerator;

void range_init(RangeGenerator* gen, int start, int end, int step) {
    gen->current = start;
    gen->end = end;
    gen->step = step;
    gen->state = 0;
}

int range_next(RangeGenerator* gen, int* value) {
    switch(gen->state) {
        case 0:
            if(gen->current >= gen->end) {
                return 0;  // Done
            }
            *value = gen->current;
            gen->current += gen->step;
            return 1;  // Has value
    }
    return 0;
}

// Primes generator using Sieve approach
typedef struct {
    int state;
    int candidate;
    int* primes;
    size_t prime_count;
    size_t prime_capacity;
} PrimeGenerator;

void prime_init(PrimeGenerator* gen) {
    gen->state = 0;
    gen->candidate = 2;
    gen->prime_count = 0;
    gen->prime_capacity = 16;
    gen->primes = malloc(gen->prime_capacity * sizeof(int));
}

int prime_next(PrimeGenerator* gen, int* value) {
    switch(gen->state) {
        case 0:  // First prime
            *value = 2;
            gen->primes[gen->prime_count++] = 2;
            gen->candidate = 3;
            gen->state = 1;
            return 1;

        case 1:  // Find next prime
            while(1) {
                int is_prime = 1;

                // Check divisibility by known primes
                for(size_t i = 0; i < gen->prime_count; i++) {
                    if(gen->candidate % gen->primes[i] == 0) {
                        is_prime = 0;
                        break;
                    }
                    // Optimization: only check up to sqrt
                    if(gen->primes[i] * gen->primes[i] > gen->candidate) {
                        break;
                    }
                }

                if(is_prime) {
                    *value = gen->candidate;

                    // Store prime for future checks
                    if(gen->prime_count >= gen->prime_capacity) {
                        gen->prime_capacity *= 2;
                        gen->primes = realloc(gen->primes,
                                            gen->prime_capacity * sizeof(int));
                    }
                    gen->primes[gen->prime_count++] = gen->candidate;

                    gen->candidate += 2;  // Skip even numbers
                    return 1;
                }

                gen->candidate += 2;
            }
    }
    return 0;
}

void prime_free(PrimeGenerator* gen) {
    free(gen->primes);
}

// Usage
PrimeGenerator gen;
prime_init(&gen);
int prime;
for(int i = 0; i < 20; i++) {
    if(prime_next(&gen, &prime)) {
        printf("%d ", prime);
    }
}
prime_free(&gen);
\end{lstlisting}

This async file reader demonstrates integrating coroutines with non-blocking I/O. The file is opened with \texttt{O\_NONBLOCK}, meaning \texttt{read()} returns immediately rather than waiting for data. If no data is available, it returns -1 with \texttt{errno} set to \texttt{EAGAIN}.

The state machine handles this explicitly. State 0 opens the file and immediately yields---even though opening might be fast, we yield for consistency. State 1 contains the main reading loop. Each iteration attempts a read. If it would block (\texttt{EAGAIN}), we yield, giving other coroutines a chance to run. The event loop will call us again later, and we'll retry the read.

This is cooperative multitasking in action. Each coroutine does a small amount of work (one read attempt) and yields. No coroutine monopolizes the CPU. The event loop gives each coroutine a chance to make progress.

When we reach EOF (\texttt{bytes\_read == 0}), we transition to the cleanup state. State 2 closes the file, reports statistics, and resets the state machine. Returning \texttt{ASYNC\_COMPLETE} tells the event loop this coroutine is done.

The event loop implementation shows how multiple coroutines run concurrently. It maintains an array of active coroutines and polls each one every iteration. Completed coroutines are removed from the array. This is vastly simpler than traditional select/epoll event loops with callback registration.

The \texttt{usleep(1000)} prevents busy-waiting. In a real implementation, you'd use \texttt{select()}, \texttt{poll()}, or \texttt{epoll()} to sleep until at least one file descriptor has data. The coroutine approach integrates naturally with these mechanisms---each coroutine represents an I/O operation, and the event loop drives them all forward.

This pattern scales to thousands of concurrent operations. Each has its own state machine tracking where it is in the I/O sequence. They all share one thread, eliminating context switch overhead and synchronization complexity. This is how servers like nginx achieve high concurrency---though they often use more sophisticated coroutine libraries rather than hand-rolled state machines.

\vspace{0.5cm}
\subsection{Async I/O Simulation}

\noindent\rule{\textwidth}{0.4pt}
\vspace{0.2cm}

Coroutines can simulate async operations without callbacks, providing a compelling alternative to traditional event-driven I/O. This is one of the most practical applications of coroutines in systems programming.

\begin{tipbox}
\textbf{The Async Advantage:} Coroutines let you write I/O code that looks synchronous but behaves asynchronously. The function appears to block at each I/O operation, but actually yields control. The result is readable, maintainable code with the efficiency of non-blocking I/O.
\end{tipbox}

\vspace{0.2cm}
\noindent\textbf{The Problem with Callbacks:} Traditional async I/O forces you to fragment your logic. Reading a file becomes: start the read, register a callback, return. When data arrives, the callback fires, processes some data, starts another read, registers another callback, and so on. Each callback is a separate function, and you must manually thread state between them.

\begin{lstlisting}
typedef struct {
    int state;
    int fd;
    char buffer[1024];
    size_t bytes_read;
    size_t total_read;
} AsyncReader;

typedef enum { ASYNC_PENDING, ASYNC_COMPLETE, ASYNC_ERROR } AsyncStatus;

AsyncStatus async_read_file(AsyncReader* reader, const char* filename) {
    switch(reader->state) {
        case 0:  // Open file
            reader->fd = open(filename, O_RDONLY | O_NONBLOCK);
            if(reader->fd < 0) {
                return ASYNC_ERROR;
            }
            reader->total_read = 0;
            reader->state = 1;
            return ASYNC_PENDING;

        case 1:  // Read chunk
            reader->bytes_read = read(reader->fd, reader->buffer,
                                     sizeof(reader->buffer));

            if(reader->bytes_read < 0) {
                if(errno == EAGAIN || errno == EWOULDBLOCK) {
                    return ASYNC_PENDING;  // Would block, yield
                }
                close(reader->fd);
                return ASYNC_ERROR;
            }

            if(reader->bytes_read == 0) {
                // EOF
                reader->state = 2;
                return ASYNC_PENDING;
            }

            // Process data
            process_data(reader->buffer, reader->bytes_read);
            reader->total_read += reader->bytes_read;

            // Continue reading
            return ASYNC_PENDING;

        case 2:  // Cleanup
            close(reader->fd);
            printf("Total read: %zu bytes\n", reader->total_read);
            reader->state = 0;
            return ASYNC_COMPLETE;
    }

    return ASYNC_ERROR;
}

// Event loop integration
void event_loop(void) {
    AsyncReader readers[MAX_READERS];
    int active_count = 0;

    // ... initialize readers ...

    while(active_count > 0) {
        for(int i = 0; i < active_count; i++) {
            AsyncStatus status = async_read_file(&readers[i], "file.txt");

            if(status == ASYNC_COMPLETE || status == ASYNC_ERROR) {
                // Remove completed reader
                readers[i] = readers[--active_count];
                i--;
            }
        }

        usleep(1000);  // Sleep briefly to avoid busy-waiting
    }
}
\end{lstlisting}

\vspace{0.5cm}
\subsection{Limitations and Considerations}

\noindent\rule{\textwidth}{0.4pt}
\vspace{0.2cm}

While coroutines are powerful, they come with significant constraints, especially in C's stackless implementations. Understanding these limitations is crucial for deciding when and how to use them.

\vspace{0.3cm}
\noindent\textbf{\large Stackless Limitations}

\vspace{0.2cm}
\noindent The techniques we've explored are ``stackless'' coroutines---they don't manipulate the actual call stack. This simplicity comes at a cost:

\begin{description}[style=nextline,leftmargin=0pt]
    \item[\textbf{Cannot preserve local variables automatically}] Every piece of state must be explicitly stored in static variables or a context structure. This is tedious and error-prone. You can't just declare a local variable and expect it to survive across yields. This is the biggest practical limitation and makes stackless coroutines feel unnatural compared to languages with native support.

    \item[\textbf{Cannot yield from nested function calls}] If your coroutine calls another function, that function cannot yield. Only the top-level coroutine function can yield. This forces you to flatten your code or pass the coroutine context to helper functions so they can modify state without yielding. It prevents the natural decomposition of complex coroutines into smaller helper functions.

    \item[\textbf{All state must be explicit}] There's no hidden magic. Every variable, every counter, every buffer must be declared in your context structure or as a static variable. This makes the state machine visible, which aids debugging, but adds significant boilerplate. You must carefully consider what state needs to persist across yields.

    \item[\textbf{Switch-based approach limits where yields can occur}] The switch statement mechanism requires yield points to be in specific places. You cannot yield inside a function call or from within certain expressions. This sometimes forces awkward code restructuring. Additionally, the technique relies on undefined behavior in some interpretations of the C standard (though it works on all practical compilers).
\end{description}

\vspace{0.4cm}
\noindent\textbf{\large Best Practices}

\begin{warningbox}
\textbf{Critical Guidelines:} Following these practices will save you from subtle bugs, memory leaks, and maintenance nightmares. Coroutines require discipline---cut corners at your peril.
\end{warningbox}

\vspace{0.2cm}

\begin{description}[style=nextline,leftmargin=0pt]
    \item[\textbf{Use for I/O-bound operations, not CPU-bound}] Coroutines shine when waiting for external events---network data, user input, file I/O. They're less useful for pure computation. If your task is CPU-intensive, coroutine overhead provides little benefit over straight-line code. The value is in managing many concurrent I/O operations efficiently.

    \item[\textbf{Keep coroutine state structures small}] Large state structures mean more memory per coroutine instance, limiting how many you can have. This matters when handling thousands of concurrent operations. Consider whether all fields are truly needed or if some can be computed on-demand.

    \item[\textbf{Document yield points clearly}] Comment each yield point explaining why you're yielding and what you expect when resumed. This helps future maintainers understand the control flow. The non-linear execution is coroutines' greatest strength and their greatest source of confusion.

    \item[\textbf{Consider thread safety}] If multiple threads might call the same coroutine, you need synchronization. Static variables in Simon Tatham's approach are particularly problematic here---they're implicitly shared. Context structure approaches are safer because each thread can have its own contexts, but you still need care if contexts are shared.

    \item[\textbf{Free allocated resources in cleanup states}] Memory leaks are easy in coroutines because resources acquired in one state might need cleanup in another. Always include explicit cleanup states, and consider what happens if a coroutine is abandoned mid-execution. In some cases, you might need a separate ``abort'' function that cleans up regardless of current state.

    \item[\textbf{Test state machine transitions thoroughly}] Every state, every transition, every error path needs testing. State machines have many more execution paths than linear code. Use unit tests that exercise all states, and consider property-based testing or state space exploration tools for critical coroutines.
\end{description}

\vspace{0.4cm}
\noindent\textbf{\large When to Use Coroutines}

\vspace{0.2cm}
\noindent Coroutines are the right tool for specific scenarios:

\begin{description}[style=nextline,leftmargin=0pt]
    \item[\textbf{Parsing complex protocols incrementally}] When you must process data as it arrives, byte by byte or packet by packet, coroutines let you write the parser as linear code rather than a tangled web of callbacks. This is perhaps their single best use case.

    \item[\textbf{Implementing generators and iterators}] Any time you need to produce a sequence of values without generating them all upfront, generators (coroutines that yield values) are ideal. This includes tree traversals, combinatorial generation, infinite sequences, and lazy evaluation.

    \item[\textbf{State machines that span multiple function calls}] If your state machine naturally wants to remember where it is across multiple invocations, coroutines are cleaner than manual state tracking. The execution point itself becomes your state.

    \item[\textbf{Cooperative task scheduling}] When you have many tasks that can make incremental progress, coroutines provide lightweight task switching. This is the foundation of many async I/O frameworks and game engines' task systems.

    \item[\textbf{Avoiding callback pyramids in async code}] When traditional callback-based async programming leads to deeply nested, hard-to-follow code, coroutines flatten the control flow. The async/await pattern in modern languages is essentially coroutines with syntactic sugar.
\end{description}

\vspace{0.4cm}
\noindent\textbf{\large Alternatives to Consider}

\begin{notebox}
\textbf{Choose Wisely:} Coroutines aren't always the answer. Each alternative has its place. Match the tool to the problem.
\end{notebox}

\vspace{0.2cm}

\begin{description}[style=nextline,leftmargin=0pt]
    \item[\textbf{Threads}] For true parallelism across CPU cores, threads are necessary. They have higher overhead (each thread needs a stack, context switching is expensive) but actually run simultaneously. Use threads for CPU-bound parallel work, coroutines for I/O-bound concurrent work.

    \item[\textbf{Callbacks}] Sometimes callbacks are simpler. For straightforward event handling with minimal state, callbacks work fine. They become problematic only when you have complex sequences of async operations. Don't reach for coroutines if a few simple callbacks suffice.

    \item[\textbf{ucontext}] POSIX provides \texttt{getcontext}, \texttt{setcontext}, \texttt{makecontext}, and \texttt{swapcontext} for stack-based context switching. These enable true stack-preserving coroutines where local variables work normally. However, this API is deprecated, non-portable (doesn't work on Windows), and tricky to use correctly. It's more powerful than stackless coroutines but fragile.

    \item[\textbf{Assembly}] You can implement coroutines in assembly by manually saving and restoring registers and manipulating stack pointers. This gives maximum control and efficiency but is architecture-specific, hard to maintain, and easy to get wrong. Only consider this for performance-critical systems code where you've exhausted all other options.

    \item[\textbf{Libraries}] Several C libraries implement coroutines: libaco (fast asymmetric coroutines), libcoro (symmetric coroutines), libtask (Plan 9-style task library), and others. These provide more features and better ergonomics than rolling your own. The cost is an external dependency and learning a library-specific API. For production use, libraries are often the right choice.
\end{description}

\vspace{0.4cm}
\begin{tipbox}
\textbf{Final Wisdom:} Coroutines in C require discipline but provide powerful abstraction for complex control flow without the overhead of operating system threads. They represent a middle ground between callbacks (simple but limiting) and threads (powerful but expensive). When you understand their constraints and use them appropriately, they can dramatically simplify systems that manage multiple concurrent operations. The key is recognizing when the benefits of sequential-looking code outweigh the costs of explicit state management.
\end{tipbox}

\vspace{0.5cm}
\noindent\rule{\textwidth}{0.4pt}

\section{Intrusive Data Structures}

Linux kernel-style intrusive containers:

\begin{lstlisting}
// Intrusive list node
typedef struct list_head {
    struct list_head *next, *prev;
} list_head;

// Initialize list
#define LIST_HEAD_INIT(name) { &(name), &(name) }
#define LIST_HEAD(name) \
    list_head name = LIST_HEAD_INIT(name)

static inline void list_init(list_head* list) {
    list->next = list;
    list->prev = list;
}

// Add to list
static inline void list_add(list_head* new_node,
                             list_head* head) {
    head->next->prev = new_node;
    new_node->next = head->next;
    new_node->prev = head;
    head->next = new_node;
}

// Remove from list
static inline void list_del(list_head* entry) {
    entry->next->prev = entry->prev;
    entry->prev->next = entry->next;
}

// Container-of magic
#define container_of(ptr, type, member) \
    ((type*)((char*)(ptr) - offsetof(type, member)))

// Iterate
#define list_for_each(pos, head) \
    for (pos = (head)->next; pos != (head); pos = pos->next)

#define list_entry(ptr, type, member) \
    container_of(ptr, type, member)

// Example usage
typedef struct {
    int id;
    char name[50];
    list_head list;  // Intrusive list node
} Person;

LIST_HEAD(people);

void add_person(int id, const char* name) {
    Person* p = malloc(sizeof(Person));
    p->id = id;
    strncpy(p->name, name, sizeof(p->name));
    list_add(&p->list, &people);
}

void print_all_people(void) {
    list_head* pos;
    list_for_each(pos, &people) {
        Person* p = list_entry(pos, Person, list);
        printf("%d: %s\n", p->id, p->name);
    }
}
\end{lstlisting}

\section{Tagged Unions (Sum Types)}

Type-safe variant types:

\begin{lstlisting}
typedef enum {
    VALUE_INT,
    VALUE_FLOAT,
    VALUE_STRING,
    VALUE_ERROR
} ValueType;

typedef struct {
    ValueType type;
    union {
        int as_int;
        double as_float;
        char* as_string;
        struct {
            int code;
            char message[100];
        } as_error;
    };
} Value;

// Type-safe constructors
Value value_int(int x) {
    return (Value){.type = VALUE_INT, .as_int = x};
}

Value value_float(double x) {
    return (Value){.type = VALUE_FLOAT, .as_float = x};
}

Value value_string(const char* s) {
    return (Value){.type = VALUE_STRING, .as_string = strdup(s)};
}

Value value_error(int code, const char* msg) {
    Value v = {.type = VALUE_ERROR};
    v.as_error.code = code;
    strncpy(v.as_error.message, msg,
            sizeof(v.as_error.message) - 1);
    return v;
}

// Pattern matching with macros
#define MATCH_VALUE(v, INT_CASE, FLOAT_CASE, STR_CASE, ERR_CASE) \
    do { \
        switch((v).type) { \
            case VALUE_INT: { \
                int _val = (v).as_int; \
                INT_CASE(_val); \
            } break; \
            case VALUE_FLOAT: { \
                double _val = (v).as_float; \
                FLOAT_CASE(_val); \
            } break; \
            case VALUE_STRING: { \
                char* _val = (v).as_string; \
                STR_CASE(_val); \
            } break; \
            case VALUE_ERROR: { \
                int _code = (v).as_error.code; \
                char* _msg = (v).as_error.message; \
                ERR_CASE(_code, _msg); \
            } break; \
        } \
    } while(0)

// Usage
Value v = compute_value();
MATCH_VALUE(v,
    INT(x)   -> printf("Int: %d\n", x),
    FLOAT(x) -> printf("Float: %f\n", x),
    STR(x)   -> printf("String: %s\n", x),
    ERR(c,m) -> printf("Error %d: %s\n", c, m)
);
\end{lstlisting}

\section{Generic Programming with Macros}

Type-safe generic containers:

\begin{lstlisting}
// Define a vector for any type
#define DEFINE_VECTOR(T) \
    typedef struct { \
        T* data; \
        size_t size; \
        size_t capacity; \
    } T##_vector; \
    \
    T##_vector* T##_vector_create(void) { \
        T##_vector* v = malloc(sizeof(T##_vector)); \
        v->data = NULL; \
        v->size = 0; \
        v->capacity = 0; \
        return v; \
    } \
    \
    void T##_vector_push(T##_vector* v, T item) { \
        if(v->size >= v->capacity) { \
            v->capacity = v->capacity ? v->capacity * 2 : 8; \
            v->data = realloc(v->data, v->capacity * sizeof(T)); \
        } \
        v->data[v->size++] = item; \
    } \
    \
    T T##_vector_get(T##_vector* v, size_t index) { \
        return v->data[index]; \
    } \
    \
    void T##_vector_destroy(T##_vector* v) { \
        free(v->data); \
        free(v); \
    }

// Generate vectors for different types
DEFINE_VECTOR(int)
DEFINE_VECTOR(float)
DEFINE_VECTOR(double)

// Usage
int_vector* iv = int_vector_create();
int_vector_push(iv, 42);
int_vector_push(iv, 100);
printf("%d\n", int_vector_get(iv, 0));
int_vector_destroy(iv);
\end{lstlisting}

\section{Reflection and Introspection}

Runtime type information in C:

\begin{lstlisting}
// Type descriptor
typedef enum {
    TYPE_INT,
    TYPE_FLOAT,
    TYPE_STRING,
    TYPE_STRUCT
} TypeKind;

typedef struct TypeInfo TypeInfo;

struct TypeInfo {
    TypeKind kind;
    const char* name;
    size_t size;

    // For structs
    struct {
        size_t field_count;
        struct {
            const char* name;
            TypeInfo* type;
            size_t offset;
        } *fields;
    } struct_info;
};

// Example: Describe a struct
typedef struct {
    int x;
    int y;
    char* name;
} Point;

TypeInfo int_type = {TYPE_INT, "int", sizeof(int)};
TypeInfo charptr_type = {TYPE_STRING, "char*", sizeof(char*)};

TypeInfo point_type = {
    .kind = TYPE_STRUCT,
    .name = "Point",
    .size = sizeof(Point),
    .struct_info = {
        .field_count = 3,
        .fields = (struct {const char* name; TypeInfo* type;
                          size_t offset;}[]){
            {"x", &int_type, offsetof(Point, x)},
            {"y", &int_type, offsetof(Point, y)},
            {"name", &charptr_type, offsetof(Point, name)},
        }
    }
};

// Generic serialization using type info
void serialize(void* obj, TypeInfo* type, FILE* f) {
    switch(type->kind) {
        case TYPE_INT:
            fprintf(f, "%d", *(int*)obj);
            break;
        case TYPE_FLOAT:
            fprintf(f, "%f", *(float*)obj);
            break;
        case TYPE_STRING:
            fprintf(f, "\"%s\"", *(char**)obj);
            break;
        case TYPE_STRUCT:
            fprintf(f, "{");
            for(size_t i = 0; i < type->struct_info.field_count; i++) {
                if(i > 0) fprintf(f, ",");
                fprintf(f, "\"%s\":",
                        type->struct_info.fields[i].name);
                void* field_ptr = (char*)obj +
                    type->struct_info.fields[i].offset;
                serialize(field_ptr,
                         type->struct_info.fields[i].type, f);
            }
            fprintf(f, "}");
            break;
    }
}
\end{lstlisting}

\section{Compile-Time Computation}

Push work to compile time:

\begin{lstlisting}
// Compute at compile time with const
static const int fibonacci[] = {
    0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144
};

// Compile-time assertions
#define COMPILE_TIME_ASSERT(cond) \
    ((void)sizeof(char[1 - 2*!(cond)]))

// Use in code
void check_assumptions(void) {
    COMPILE_TIME_ASSERT(sizeof(int) == 4);
    COMPILE_TIME_ASSERT(sizeof(void*) == 8);
    COMPILE_TIME_ASSERT(sizeof(long) >= sizeof(int));
}

// Constexpr-like behavior (C11)
#define ARRAY_SIZE 100
static const size_t buffer_size = ARRAY_SIZE * sizeof(int);
char buffer[buffer_size]; // Compile-time computation
\end{lstlisting}

\section{Continuation Passing Style}

\begin{lstlisting}
// CPS transforms control flow into data
typedef void (*Continuation)(void* result, void* context);

void async_read_file(const char* path,
                     Continuation cont,
                     void* context) {
    // Start async read
    // When done, call: cont(data, context);
}

void on_file_read(void* result, void* context) {
    char* data = (char*)result;
    printf("File contents: %s\n", data);
    free(data);
}

// Chain continuations
void step1_done(void* result, void* context) {
    printf("Step 1 complete\n");
    async_read_file("file.txt", step2_done, context);
}

void step2_done(void* result, void* context) {
    printf("Step 2 complete\n");
    // Continue chain...
}
\end{lstlisting}

\section{Object System}

Minimal object-oriented system:

\begin{lstlisting}
// Base object with vtable
typedef struct Class Class;
typedef struct Object Object;

struct Class {
    const char* name;
    size_t size;
    void (*constructor)(Object* self);
    void (*destructor)(Object* self);
    char* (*to_string)(Object* self);
};

struct Object {
    Class* class;
    int ref_count;
};

// Object operations
Object* object_new(Class* class) {
    Object* obj = calloc(1, class->size);
    obj->class = class;
    obj->ref_count = 1;
    if(class->constructor) {
        class->constructor(obj);
    }
    return obj;
}

void object_retain(Object* obj) {
    obj->ref_count++;
}

void object_release(Object* obj) {
    if(--obj->ref_count == 0) {
        if(obj->class->destructor) {
            obj->class->destructor(obj);
        }
        free(obj);
    }
}

// Example class
typedef struct {
    Object base;
    int value;
} Integer;

void integer_constructor(Object* self) {
    Integer* i = (Integer*)self;
    i->value = 0;
}

char* integer_to_string(Object* self) {
    Integer* i = (Integer*)self;
    char* str = malloc(20);
    sprintf(str, "%d", i->value);
    return str;
}

Class IntegerClass = {
    .name = "Integer",
    .size = sizeof(Integer),
    .constructor = integer_constructor,
    .destructor = NULL,
    .to_string = integer_to_string
};

// Usage
Integer* num = (Integer*)object_new(&IntegerClass);
num->value = 42;
char* str = num->base.class->to_string((Object*)num);
printf("%s\n", str);
free(str);
object_release((Object*)num);
\end{lstlisting}

\section{Zero-Cost Abstractions}

Macros that compile to optimal code:

\begin{lstlisting}
// Optional type that optimizes away
#define OPTION(T) \
    struct { \
        int has_value; \
        T value; \
    }

#define SOME(x) {1, (x)}
#define NONE {0}

#define IS_SOME(opt) ((opt).has_value)
#define UNWRAP(opt) ((opt).value)

// Usage
OPTION(int) maybe_divide(int a, int b) {
    if(b == 0) {
        OPTION(int) result = NONE;
        return result;
    }
    OPTION(int) result = SOME(a / b);
    return result;
}

OPTION(int) result = maybe_divide(10, 2);
if(IS_SOME(result)) {
    printf("Result: %d\n", UNWRAP(result));
}

// Compiles to simple branch, no overhead!
\end{lstlisting}

\section{Aspect-Oriented Programming}

Cross-cutting concerns with macros:

\begin{lstlisting}
// Automatic logging
#define LOGGED_FUNCTION(ret, name, ...) \
    ret _logged_##name(__VA_ARGS__); \
    ret name(__VA_ARGS__) { \
        printf("[CALL] %s\n", #name); \
        ret result = _logged_##name(__VA_ARGS__); \
        printf("[RETURN] %s\n", #name); \
        return result; \
    } \
    ret _logged_##name(__VA_ARGS__)

// Use it
LOGGED_FUNCTION(int, add, int a, int b) {
    return a + b;
}

// Expands to function with automatic logging
// add(5, 3) prints:
// [CALL] add
// [RETURN] add

// Timing decorator
#define TIMED_FUNCTION(ret, name, ...) \
    ret _timed_##name(__VA_ARGS__); \
    ret name(__VA_ARGS__) { \
        clock_t start = clock(); \
        ret result = _timed_##name(__VA_ARGS__); \
        clock_t end = clock(); \
        printf("%s took %.6f seconds\n", #name, \
               (double)(end - start) / CLOCKS_PER_SEC); \
        return result; \
    } \
    ret _timed_##name(__VA_ARGS__)
\end{lstlisting}

\section{Memory Pools: Custom Allocators}

Sometimes malloc/free are too slow or cause fragmentation. Memory pools to the rescue:

\begin{lstlisting}
// Fixed-size object pool
typedef struct Pool Pool;

struct Pool {
    void* memory;
    size_t object_size;
    size_t capacity;
    size_t count;
    void** free_list;
};

Pool* pool_create(size_t object_size, size_t capacity) {
    Pool* pool = malloc(sizeof(Pool));
    pool->object_size = object_size;
    pool->capacity = capacity;
    pool->count = 0;

    // Allocate memory block
    pool->memory = malloc(object_size * capacity);

    // Build free list
    pool->free_list = malloc(sizeof(void*) * capacity);
    for(size_t i = 0; i < capacity; i++) {
        pool->free_list[i] = (char*)pool->memory +
                              (i * object_size);
    }

    return pool;
}

void* pool_alloc(Pool* pool) {
    if(pool->count >= pool->capacity) {
        return NULL; // Pool exhausted
    }
    return pool->free_list[pool->count++];
}

void pool_free(Pool* pool, void* ptr) {
    if(pool->count == 0) return;
    pool->free_list[--pool->count] = ptr;
}

void pool_destroy(Pool* pool) {
    free(pool->memory);
    free(pool->free_list);
    free(pool);
}

// Usage: Lightning-fast allocation
typedef struct { int x, y, z; } Particle;

Pool* particle_pool = pool_create(sizeof(Particle), 10000);

Particle* p1 = pool_alloc(particle_pool);
Particle* p2 = pool_alloc(particle_pool);
// No malloc overhead!

pool_free(particle_pool, p1);
pool_free(particle_pool, p2);
\end{lstlisting}

\subsection{Arena Allocator: Bulk Deallocation}

\begin{lstlisting}
// Allocate many objects, free all at once
typedef struct {
    char* buffer;
    size_t size;
    size_t used;
} Arena;

Arena* arena_create(size_t size) {
    Arena* arena = malloc(sizeof(Arena));
    arena->buffer = malloc(size);
    arena->size = size;
    arena->used = 0;
    return arena;
}

void* arena_alloc(Arena* arena, size_t size) {
    // Align to 8 bytes
    size = (size + 7) & ~7;

    if(arena->used + size > arena->size) {
        return NULL; // Arena full
    }

    void* ptr = arena->buffer + arena->used;
    arena->used += size;
    return ptr;
}

void arena_reset(Arena* arena) {
    arena->used = 0; // Free everything!
}

void arena_destroy(Arena* arena) {
    free(arena->buffer);
    free(arena);
}

// Perfect for per-request data in servers
Arena* request_arena = arena_create(1024 * 1024); // 1MB

while(handle_request()) {
    // Allocate tons of temporary data
    char* buffer = arena_alloc(request_arena, 4096);
    Node* tree = arena_alloc(request_arena, sizeof(Node));

    // Process request...

    // Free everything instantly!
    arena_reset(request_arena);
}
\end{lstlisting}

\section{Plugin Systems: Dynamic Loading}

Build extensible applications with runtime plugin loading:

\begin{lstlisting}
// Plugin interface
typedef struct {
    const char* name;
    const char* version;
    int (*init)(void);
    void (*shutdown)(void);
    void (*process)(void* data);
} Plugin;

// Plugin loader
#ifdef _WIN32
#include <windows.h>
typedef HMODULE PluginHandle;
#define LOAD_PLUGIN(path) LoadLibrary(path)
#define GET_SYMBOL(handle, name) GetProcAddress(handle, name)
#define CLOSE_PLUGIN(handle) FreeLibrary(handle)
#else
#include <dlfcn.h>
typedef void* PluginHandle;
#define LOAD_PLUGIN(path) dlopen(path, RTLD_LAZY)
#define GET_SYMBOL(handle, name) dlsym(handle, name)
#define CLOSE_PLUGIN(handle) dlclose(handle)
#endif

typedef struct {
    PluginHandle handle;
    Plugin* plugin;
} LoadedPlugin;

LoadedPlugin load_plugin(const char* path) {
    LoadedPlugin loaded = {0};

    loaded.handle = LOAD_PLUGIN(path);
    if(!loaded.handle) {
        fprintf(stderr, "Failed to load plugin: %s\n", path);
        return loaded;
    }

    // Get plugin descriptor
    Plugin* (*get_plugin)(void) = GET_SYMBOL(loaded.handle,
                                              "get_plugin");
    if(!get_plugin) {
        fprintf(stderr, "Plugin missing get_plugin()\n");
        CLOSE_PLUGIN(loaded.handle);
        loaded.handle = NULL;
        return loaded;
    }

    loaded.plugin = get_plugin();

    if(loaded.plugin->init) {
        if(loaded.plugin->init() != 0) {
            fprintf(stderr, "Plugin init failed\n");
            CLOSE_PLUGIN(loaded.handle);
            loaded.handle = NULL;
            return loaded;
        }
    }

    printf("Loaded plugin: %s v%s\n",
           loaded.plugin->name, loaded.plugin->version);

    return loaded;
}

void unload_plugin(LoadedPlugin* loaded) {
    if(loaded->handle) {
        if(loaded->plugin && loaded->plugin->shutdown) {
            loaded->plugin->shutdown();
        }
        CLOSE_PLUGIN(loaded->handle);
        loaded->handle = NULL;
        loaded->plugin = NULL;
    }
}

// Example plugin implementation (in separate .so/.dll)
int my_plugin_init(void) {
    printf("My plugin initializing\n");
    return 0;
}

void my_plugin_shutdown(void) {
    printf("My plugin shutting down\n");
}

void my_plugin_process(void* data) {
    printf("Processing: %s\n", (char*)data);
}

Plugin my_plugin = {
    .name = "MyPlugin",
    .version = "1.0",
    .init = my_plugin_init,
    .shutdown = my_plugin_shutdown,
    .process = my_plugin_process
};

Plugin* get_plugin(void) {
    return &my_plugin;
}
\end{lstlisting}

\section{Domain-Specific Languages (DSLs)}

Create mini-languages for specific tasks:

\begin{lstlisting}
// Simple expression DSL
// Example: "x + y * 2" or "max(a, b + c)"

typedef enum {
    TOKEN_NUMBER,
    TOKEN_IDENT,
    TOKEN_PLUS,
    TOKEN_MINUS,
    TOKEN_STAR,
    TOKEN_SLASH,
    TOKEN_LPAREN,
    TOKEN_RPAREN,
    TOKEN_COMMA,
    TOKEN_EOF
} TokenType;

typedef struct {
    TokenType type;
    union {
        double number;
        char ident[32];
    };
} Token;

// Tokenizer
typedef struct {
    const char* input;
    size_t pos;
    Token current;
} Lexer;

void lexer_init(Lexer* lex, const char* input) {
    lex->input = input;
    lex->pos = 0;
}

void lexer_next(Lexer* lex) {
    // Skip whitespace
    while(isspace(lex->input[lex->pos])) lex->pos++;

    char c = lex->input[lex->pos];

    if(c == '\0') {
        lex->current.type = TOKEN_EOF;
        return;
    }

    if(isdigit(c)) {
        char* end;
        lex->current.type = TOKEN_NUMBER;
        lex->current.number = strtod(lex->input + lex->pos, &end);
        lex->pos = end - lex->input;
        return;
    }

    if(isalpha(c)) {
        lex->current.type = TOKEN_IDENT;
        size_t i = 0;
        while(isalnum(lex->input[lex->pos]) && i < 31) {
            lex->current.ident[i++] = lex->input[lex->pos++];
        }
        lex->current.ident[i] = '\0';
        return;
    }

    // Operators
    lex->pos++;
    switch(c) {
        case '+': lex->current.type = TOKEN_PLUS; break;
        case '-': lex->current.type = TOKEN_MINUS; break;
        case '*': lex->current.type = TOKEN_STAR; break;
        case '/': lex->current.type = TOKEN_SLASH; break;
        case '(': lex->current.type = TOKEN_LPAREN; break;
        case ')': lex->current.type = TOKEN_RPAREN; break;
        case ',': lex->current.type = TOKEN_COMMA; break;
    }
}

// Simple recursive descent parser
typedef struct Expr Expr;

struct Expr {
    enum { EXPR_NUM, EXPR_VAR, EXPR_BINOP, EXPR_CALL } type;
    union {
        double number;
        char var[32];
        struct {
            char op;
            Expr *left, *right;
        } binop;
        struct {
            char func[32];
            Expr** args;
            int arg_count;
        } call;
    };
};

// Parse and evaluate
double eval(Expr* expr, double* vars) {
    switch(expr->type) {
        case EXPR_NUM:
            return expr->number;
        case EXPR_VAR:
            // Look up variable (simplified)
            return vars[expr->var[0] - 'a'];
        case EXPR_BINOP: {
            double left = eval(expr->binop.left, vars);
            double right = eval(expr->binop.right, vars);
            switch(expr->binop.op) {
                case '+': return left + right;
                case '-': return left - right;
                case '*': return left * right;
                case '/': return left / right;
            }
        }
        case EXPR_CALL:
            // Function calls (simplified)
            if(strcmp(expr->call.func, "max") == 0) {
                double a = eval(expr->call.args[0], vars);
                double b = eval(expr->call.args[1], vars);
                return a > b ? a : b;
            }
            break;
    }
    return 0;
}

// Usage
// Parse "x + y * 2" and evaluate with x=5, y=3
// Result: 5 + 3*2 = 11
\end{lstlisting}

\section{Finite State Transducers}

Beyond state machines---transform input to output:

\begin{lstlisting}
// FST: Transform input sequence to output sequence
typedef struct {
    int state;
    int input;
    int output;
    int next_state;
} Transition;

typedef struct {
    Transition* transitions;
    int transition_count;
    int current_state;
} FST;

// Example: Convert "hello" to "HELLO"
Transition uppercase_fst[] = {
    {0, 'h', 'H', 0},
    {0, 'e', 'E', 0},
    {0, 'l', 'L', 0},
    {0, 'o', 'O', 0},
    // ... more transitions
};

int fst_process(FST* fst, int input) {
    for(int i = 0; i < fst->transition_count; i++) {
        Transition* t = &fst->transitions[i];
        if(t->state == fst->current_state &&
           t->input == input) {
            fst->current_state = t->next_state;
            return t->output;
        }
    }
    return -1; // No transition
}

// More complex: Phone number formatter
// Input: "5551234567"
// Output: "(555) 123-4567"
\end{lstlisting}

\section{Visitor Pattern in C}

Object-oriented visitor pattern without classes:

\begin{lstlisting}
// Abstract syntax tree
typedef struct Node Node;

struct Node {
    enum { NODE_NUM, NODE_ADD, NODE_MUL } type;
    union {
        int number;
        struct { Node *left, *right; } binop;
    };
};

// Visitor interface
typedef struct {
    void (*visit_num)(int value, void* context);
    void (*visit_add)(Node* left, Node* right, void* context);
    void (*visit_mul)(Node* left, Node* right, void* context);
} Visitor;

void node_accept(Node* node, Visitor* visitor, void* context) {
    switch(node->type) {
        case NODE_NUM:
            visitor->visit_num(node->number, context);
            break;
        case NODE_ADD:
            node_accept(node->binop.left, visitor, context);
            node_accept(node->binop.right, visitor, context);
            visitor->visit_add(node->binop.left, node->binop.right,
                              context);
            break;
        case NODE_MUL:
            node_accept(node->binop.left, visitor, context);
            node_accept(node->binop.right, visitor, context);
            visitor->visit_mul(node->binop.left, node->binop.right,
                              context);
            break;
    }
}

// Example visitor: Pretty printer
void print_num(int value, void* ctx) {
    printf("%d", value);
}

void print_add(Node* left, Node* right, void* ctx) {
    printf(" + ");
}

void print_mul(Node* left, Node* right, void* ctx) {
    printf(" * ");
}

Visitor printer = {
    .visit_num = print_num,
    .visit_add = print_add,
    .visit_mul = print_mul
};

// Example visitor: Evaluator
void eval_num(int value, void* ctx) {
    int* result = (int*)ctx;
    *result = value;
}

void eval_add(Node* left, Node* right, void* ctx) {
    int left_val, right_val;
    node_accept(left, &evaluator, &left_val);
    node_accept(right, &evaluator, &right_val);
    *(int*)ctx = left_val + right_val;
}
\end{lstlisting}

\section{Summary}

You've now seen the deep magic of C:

\begin{itemize}
    \item \textbf{X-Macros}: Maintainable code generation without external tools
    \item \textbf{Coroutines}: Cooperative multitasking without threads
    \item \textbf{Intrusive Structures}: Linux kernel-style zero-overhead containers
    \item \textbf{Tagged Unions}: Type-safe variant types
    \item \textbf{Generic Programming}: Type-safe generics through macros
    \item \textbf{Reflection}: Runtime type information in C
    \item \textbf{Compile-Time Computation}: Push work to the compiler
    \item \textbf{Continuation Passing}: Transform control flow to data
    \item \textbf{Object Systems}: OOP when you need it
    \item \textbf{Zero-Cost Abstractions}: High-level code, low-level performance
    \item \textbf{Aspect-Oriented}: Cross-cutting concerns through macros
    \item \textbf{Memory Pools}: Custom allocators for performance
    \item \textbf{Plugin Systems}: Runtime extensibility
    \item \textbf{DSLs}: Domain-specific languages embedded in C
    \item \textbf{FSTs}: Finite state transducers for transformations
    \item \textbf{Visitor Pattern}: Object-oriented patterns without objects
\end{itemize}

\subsection{The Art of Advanced C}

These patterns aren't tricks---they're techniques. Each solves real problems:

\begin{itemize}
    \item Use X-Macros when you have parallel data structures
    \item Use intrusive containers when performance matters
    \item Use memory pools for predictable allocation
    \item Use plugins for extensible architectures
    \item Use DSLs when configuration isn't enough
    \item Use visitors when operations vary more than types
\end{itemize}

\subsection{When to Use Advanced Patterns}

\textbf{Always:}
\begin{itemize}
    \item X-Macros for enums with string names
    \item Tagged unions for variant types
    \item Compile-time assertions
\end{itemize}

\textbf{Often:}
\begin{itemize}
    \item Generic programming with macros
    \item Intrusive data structures in performance code
    \item Memory pools in real-time systems
\end{itemize}

\textbf{Sometimes:}
\begin{itemize}
    \item Coroutines for state machines
    \item Reflection for serialization
    \item Plugin systems for extensibility
\end{itemize}

\textbf{Rarely:}
\begin{itemize}
    \item Full object systems (just use C++)
    \item Continuation passing (confusing for most)
    \item DSLs (big maintenance burden)
\end{itemize}

\subsection{Final Thoughts on Advanced Patterns}

C is simple, but not simplistic. It provides just enough to build sophisticated abstractions while staying close to the metal. These patterns show that C can express complex ideas---but should you?

The best C code is:
\begin{enumerate}
    \item \textbf{Obvious}: Someone reading it understands it quickly
    \item \textbf{Efficient}: It doesn't waste resources
    \item \textbf{Maintainable}: Future you can modify it without fear
    \item \textbf{Appropriate}: The complexity matches the problem
\end{enumerate}

Don't use advanced patterns to show off. Use them to solve problems. The cleverest C code isn't the most complex---it's the simplest code that does the job right.

Now you have the full arsenal of C techniques. Use them wisely, use them well, and remember: just because you \textit{can} build a coroutine-based intrusive generic reflection system doesn't mean you \textit{should}.

Master the patterns, but master restraint too. That's the true art of C programming!
